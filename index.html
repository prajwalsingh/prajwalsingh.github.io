
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

a {
color: #1772d0;
text-decoration: none;
}

a:focus,
a:hover {
color: #f09228;
text-decoration: none;
}

body,
td,
th,
tr,
p,
a {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
text-align: justify;
text-justify: inter-word;
}

strong {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
}

heading {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 24px;
text-decoration: underline;
}

papertitle {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
font-weight: 700
}

name {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 36px;
}

.one {
width: 160px;
height: 160px;
position: relative;
}

.two {
width: 160px;
height: 160px;
position: absolute;
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

.fade {
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

span.highlight {
background-color: #ffffd0;
}
  </style>
<!--   <link rel="icon" type="image/png" href="img/avtr.png"> -->
  <title>Prajwal Singh</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tr><td colspan="2">
	        <table align="center" width="100%" border="0" cellspacing="0" cellpadding="20">
	        <tr>
		 <td valign="middle" colspan="2">
			<p align="center">
			  <name>Prajwal Kumar Singh</name>
			</p>
			<p>
			  I am a Computer Science Ph.D. student at IIT Gandhinagar, India, advised by 
			  <a href="https://people.iitgn.ac.in/~shanmuga/" target="_blank">Prof. Shanmuganathan Raman</a>. My research interests include 3D computer vision, generative networks, graph neural networks,
			  representation learning and brain-computer interface (BCI).
			</p>
			<p align=center>
			  <a href="mailto:singh_prajwal@iitgn.ac.in">Email</a> &nbsp/&nbsp
			  <a href="https://scholar.google.co.in/citations?user=w5GbFHIAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
			  <a href="https://github.com/prajwalsingh" target="_blank">GitHub</a> &nbsp/&nbsp
			  <a href="https://www.linkedin.com/in/prajwalsingh15/" target="_blank">LinkedIn</a> &nbsp/&nbsp
			  <a href="https://twitter.com/prajwal_15" target="_blank">Twitter</a> &nbsp/&nbsp
			  <a href="https://www.youtube.com/@PrajwalSingh15" target="_blank">YoutTube</a> &nbsp/&nbsp
			  <a href="https://drive.google.com/file/d/1k9TLBCgWG6r26blE_Sx6Wyty6kXbjmQI/view?usp=sharing" target="_blank">CV</a> &nbsp/&nbsp
			  <a href="https://htmlpreview.github.io/?https://github.com/prajwalsingh/Python-Notes/blob/master/index.html" target="_blank">Python Notes</a>	
			</p>
		  </td>
		<td></td>
		</tr>
		</table>
       </td>
       <td></td>      
       </tr>

      <tr><td colspan="2">
      <!-- Research Publications -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publications</heading>
        </td>
      </tr>
      </table>
      </td>
      <td></td>
      </tr>

	<!-- Handshadow Art -->
	      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
		<td style="padding:25px;width:25%;vertical-align:middle">
		  <div class="one">
		    <div class="two" id='dd_image'><img src='gifs/handshadow.png' width="160"></div>
		    <img src='gifs/handshadow.png' width="160">
		  </div>
		</td>
		<td style="padding:20px;width:75%;vertical-align:middle">
		  <a href="https://diglib.eg.org/bitstream/handle/10.2312/pg20231279/107-108.pdf" target="_blank">
		    <papertitle>Hand Shadow Art: A Differentiable Rendering Perspective</papertitle>
		  </a>
		  <be>
		  <a href="#" target="_blank">Aalok Gangopadhyay</a>,
		  <strong>Prajwal Singh</strong>,
		  <a href="https://sites.google.com/iitgn.ac.in/ashishtiwari" target="_blank">Ashish Tiwari</a>,
		  <a href="http://people.iitgn.ac.in/~shanmuga/" target="_blank">Shanmuganathan Raman</a>,
		  <br>
		  <em>PG2023-Poster</em>, 2023
		  <br>
		  <a href="#">project page</a>/
		  <a href="#">arXiv</a>/
		  <a href="https://diglib.eg.org/bitstream/handle/10.2312/pg20231279/107-108.pdf">PG2023</a>
		  <p></p>
		  <p> This work introduces a novel approach using differentiable rendering to deform hand models, enabling the creation of meaningful and consistent shadows resembling desired target images.
		  </p>
		</td>
	      </tr>

	<!-- TreeGCN-ED -->
	      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
		<td style="padding:25px;width:25%;vertical-align:middle">
		  <div class="one">
		    <div class="two" id='dd_image'><img src='gifs/TreeGCNED.gif' width="160"></div>
		    <img src='gifs/TreeGCNED.gif' width="160">
		  </div>
		</td>
		<td style="padding:20px;width:75%;vertical-align:middle">
		  <a href="https://diglib.eg.org/bitstream/handle/10.2312/pg20231278/105-106.pdf" target="_blank">
		    <papertitle>TreeGCN-ED: Encoding Point Cloud using a Tree-Structured Graph Network</papertitle>
		  </a>
		  <br>
		  <strong>Prajwal Singh</strong>,
		  <a href="https://sites.google.com/iitgn.ac.in/ashishtiwari" target="_blank">Ashish Tiwari</a>,
		  <a href="https://kaustubh-sadekar.github.io/" target="_blank">Kaustubh Sadekar</a>,
		  <a href="http://people.iitgn.ac.in/~shanmuga/" target="_blank">Shanmuganathan Raman</a>,
		  <br>
		  <em>PG2023-Poster</em>, 2023
		  <br>
		  <a href="https://github.com/prajwalsingh/TreeGCN-ED">project page</a>/
		  <a href="https://arxiv.org/abs/2110.03170">arXiv</a>/
		  <a href="https://diglib.eg.org/bitstream/handle/10.2312/pg20231278/105-106.pdf">PG2023</a>
		  <p></p>
		  <p>This work proposes a tree-structured autoencoder framework to generate robust embeddings of point clouds by utilizing hierarchical information using graph convolution. We perform multiple experiments to assess the quality of embeddings generated by the proposed encoder architecture and visualize the t-SNE map to highlight its ability to distinguish between different object classes. 
		  </p>
		</td>
	      </tr>
	  
      <!-- LDRHDR -->
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:25px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='dd_image' valign="middle"><img src='gifs/ldrhdr.png' width="160"></div>
              <img src='gifs/ldrhdr.png' width="160">
            </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2302.10121">
            <papertitle>Single Image LDR to HDR Conversion using Conditional Diffusion</papertitle>
          </a>
          <br> 
	<a href="#" target="_blank">Dwip Dalal</a>,
	<a href="#" target="_blank">Gautam Vashishtha</a>,
	<strong>Prajwal Singh</strong>,
        <a href="http://people.iitgn.ac.in/~shanmuga/" target="_blank">Shanmuganathan Raman</a>
        <br>
        <em>ICIP</em>, 2023
        <br>
        <a href="#">project page</a>/
        <a href="https://arxiv.org/abs/2307.02814">arXiv</a>/
        <a href="#">video</a>
        <p></p>
        <p> We formulate the problem as an image-to-image (I2I) translation task and propose a conditional Denoising Diffusion Probabilistic Model (DDPM) based framework using classifier-free guidance. We incorporate a deep CNN-based autoencoder in our proposed framework to enhance the quality of the latent representation of the input LDR image used for conditioning.
        </p>
        </td>
      </tr>
	  
      <!-- EEG2Image -->
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:25px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='dd_image' valign="middle"><img src='gifs/EEG2Image_Architecture.png' width="160"></div>
              <img src='gifs/EEG2Image_Architecture.png' width="160">
            </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2302.10121">
            <papertitle>EEG2IMAGE: Image Reconstruction from EEG Brain Signals</papertitle>
          </a>
          <br> 
        <strong>Prajwal Singh</strong>,
	<a href="#" target="_blank">Pankaj Pandey</a>,
	<a href="https://iitgn.ac.in/faculty/cse/krishna" target="_blank">Krishna Miyapuram</a>,
        <a href="http://people.iitgn.ac.in/~shanmuga/" target="_blank">Shanmuganathan Raman</a>
        <br>
        <em>ICASSP</em>, 2023
        <br>
        <a href="https://github.com/prajwalsingh/EEG2Image">project page</a>/
        <a href="https://arxiv.org/abs/2302.10121">arXiv</a>/
        <a href="#">video</a>
        <p></p>
        <p> We use a contrastive learning method in the proposed framework to extract features from EEG signals and synthesize the images from extracted features using conditional GAN. We modify the loss function to train the GAN, which enables it to synthesize 128x128 images using a small number of images.
        </p>
        </td>
      </tr>
	  
	  
      <!-- APEx-Net -->
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:25px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='dd_image' valign="middle"><img src='gifs/apexnet.png' width="160"></div>
              <img src='gifs/apexnet.png' width="160">
            </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/abstract/document/9806720">
            <papertitle>APEX-Net: Automatic Plot Extraction Network</papertitle>
          </a>
          <br>
	<a href="#" target="_blank">Aalok Gangopadhyay</a>, 
        <strong>Prajwal Singh</strong>,
        <a href="http://people.iitgn.ac.in/~shanmuga/" target="_blank">Shanmuganathan Raman</a>
        <br>
        <em>NCC</em>, 2022
        <br>
        <a href="#">project page</a>/
        <a href="https://arxiv.org/abs/2101.06217">arXiv</a>/
        <a href="#">video</a>
        <p></p>
        <p> To minimize this intervention, we propose APEX-Net, a deep learning based framework with novel loss functions for solving the plot extraction problem. We introduce APEX-1M, a new large scale dataset which contains both the plot images and the raw data.
        </p>
        </td>
      </tr>

      <!-- LS-HDIB -->
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:25px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='dd_image'><img src='gifs/pgNet.gif' width="160"></div>
              <img src='gifs/pgNet.gif' width="160">
            </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2101.11674">
            <papertitle>LS-HDIB: A Large Scale Handwritten Document Image Binarization Dataset</papertitle>
          </a>
          <br>
	<a href="https://kaustubh-sadekar.github.io/" target="_blank">Kaustubh Sadekar</a>, 
        <a href="https://sites.google.com/iitgn.ac.in/ashishtiwari/" target="_blank">Ashish Tiwari</a>,
        <strong>Prajwal Singh</strong>,
        <a href="http://people.iitgn.ac.in/~shanmuga/" target="_blank">Shanmuganathan Raman</a>
        <br>
        <em>ICPR</em>, 2022
        <br>
        <a href="https://kaustubh-sadekar.github.io/LS-HDIB/">project page</a>/
        <a href="https://arxiv.org/abs/2101.11674">arXiv</a>/
        <a href="#">video</a>
        <p></p>
        <p>We propose a large scale dataset of 1 Million challenging images for the task of hand written document image binarisation (HDIB) with accurate segmentation groundtruth.
        </p>
        </td>
      </tr>
	  
      <!-- DILIE -->
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:25px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='dd_image' valign="middle"><img src='gifs/dilie.png' width="160"></div>
              <img src='gifs/dilie.png' width="160">
            </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/WACV2022W/VAQ/papers/Mastan_DILIE_Deep_Internal_Learning_for_Image_Enhancement_WACVW_2022_paper.pdf">
            <papertitle>DILIE: deep internal learning for image enhancement</papertitle>
          </a>
          <br>
	<a href="#" target="_blank">Indradeep Mastan</a>,
        <a href="http://people.iitgn.ac.in/~shanmuga/" target="_blank">Shanmuganathan Raman</a>,
	<strong>Prajwal Singh</strong>
        <br>
        <em>WACV VAQ Workshop</em>, 2022
        <br>
        <a href="#">project page</a>/
        <a href="#">arXiv</a>/
        <a href="#">video</a>
        <p></p>
        <p> We perform image enhancement using a deep internal learning framework. Our Deep Internal Learning for Image Enhancement framework (DILIE) enhances content features and style features and preserves semantics in the enhanced image.
        </p>
        </td>
      </tr>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          Thanks to <a href="https://jonbarron.info" >Jon Barron</a> for this awesome template and <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a> for additional formatting.<br>
          Last updated July 2023.
	    </font>
        </p>
        </td>
      </tr>
    </table>

 </body>
</html>
